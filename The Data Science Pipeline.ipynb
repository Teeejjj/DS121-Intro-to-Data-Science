{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a162ccfd-9fa4-4efb-ba5a-7be0fcfdd534",
   "metadata": {},
   "source": [
    "# **The Data Science Pipeline**\n",
    "\n",
    "**Instructors:** Jhun Brian M. Andam | Timothy Jonah Borromeo\n",
    "\n",
    "**Course:** Introduction to Data Science\n",
    "\n",
    "**Objectives**\n",
    "\n",
    "- Understand the ins and outs of the data science process and its components.\n",
    "- Design and build your own data science solution and pipeline for a particular problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43b3642a-973c-43aa-afb7-ba8284b27b9a",
   "metadata": {},
   "source": [
    "> **Think Back**\n",
    "\n",
    "<center><img src=\"../four_acts.png\" width=\"600px\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36cd4d2a-b663-43f4-a90c-89e9eb88ea34",
   "metadata": {},
   "source": [
    "## **What is the Data Science Pipeline?**\n",
    "\n",
    "The **Data Science Pipeline** is a step-by-step workflow that guides the entire process of solving a problem using data. It helps data scientists systematically approach complex tasks, from defining the problem to deploying a final model and monitoring its performance. Though variations exist, most pipelines share common stages:\n",
    "\n",
    "**1. Problem Definition**\n",
    "Every data science project begins with a clear understanding of the problem:\n",
    "- What is the business or research question?\n",
    "- What are the goals and constraints?\n",
    "- What will success look like (e.g., a certain accuracy or business KPI improvement)?\n",
    "\n",
    "**2. Data Collection**\n",
    "Once the problem is understood, relevant data must be gathered:\n",
    "- **Sources**: databases, CSV files, APIs, IoT devices, web scraping, user logs.\n",
    "- **Formats**: structured (tables), semi-structured (JSON, XML), unstructured (text, images).\n",
    "- Ethical concerns and compliance (e.g., GDPR) should be addressed at this stage.\n",
    "\n",
    "**3. Data Preprocessing**\n",
    "Raw data is often messy. Preprocessing ensures the data is usable:\n",
    "- **Cleaning**: handling missing values, removing duplicates, correcting data types.\n",
    "- **Transformation**: encoding categorical variables, normalizing or scaling numerical data.\n",
    "- **Exploration**: using visualizations and statistics to understand distributions, relationships, and outliers.\n",
    "\n",
    "**4. Feature Engineering**\n",
    "Features are the variables used to train models. This stage includes:\n",
    "- Selecting the most relevant variables.\n",
    "- Creating new variables (e.g., ratios, time lags, interaction terms).\n",
    "- Reducing dimensionality to prevent overfitting and improve performance.\n",
    "\n",
    "**5. Model Building**\n",
    "Here, predictive or descriptive models are developed:\n",
    "- **Model selection**: regression, classification, clustering, time series models, etc.\n",
    "- **Training**: fitting the model to the training data.\n",
    "- **Hyperparameter tuning**: optimizing model parameters using grid search, random search, etc.\n",
    "\n",
    "**6. Model Evaluation**\n",
    "After training, the model is evaluated to assess performance:\n",
    "- **Metrics**: accuracy, precision, recall, F1-score, ROC-AUC, MAE, RMSE, etc.\n",
    "- **Validation techniques**: cross-validation, holdout sets.\n",
    "- This step helps detect issues like **overfitting** (model performs well on training data but poorly on new data).\n",
    "\n",
    "**7. Model Deployment**\n",
    "Once a model is performing well, it must be integrated into a real-world system:\n",
    "- **Deployment options**: REST APIs, cloud services, edge devices.\n",
    "- **Tools**: Flask, FastAPI, Docker, AWS/GCP/Azure.\n",
    "- Often involves automation pipelines like CI/CD for smoother updates.\n",
    "\n",
    "**8. Monitoring and Maintenance**\n",
    "Even after deployment, work isn't finished:\n",
    "- **Monitoring** for model drift, data quality issues, or changing input distributions.\n",
    "- **Retraining** models as new data becomes available.\n",
    "- Ensuring long-term performance, reliability, and accountability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32e5fd14-e371-4c88-95cf-534a5d5d6b94",
   "metadata": {},
   "source": [
    "A structured data science workflow is essential because it **provides a systematic approach to solving problems**, ensuring that each stage—from problem formulation to model deployment—is executed with clarity and consistency. By following a defined pipeline, data scientists can better align their technical efforts with business objectives, reduce errors, and promote collaboration across multidisciplinary teams. For instance, in a healthcare analytics project aimed at predicting patient readmissions, skipping or rushing through data preprocessing could result in biased models due to missing values or imbalanced classes. However, by adhering to a structured pipeline, the team can methodically clean the data, engineer meaningful features like hospital stay duration or comorbidity scores, and properly evaluate model performance using stratified cross-validation. This approach not only improves model reliability but also builds trust among stakeholders like clinicians and hospital administrators. Moreover, structured workflows facilitate documentation, making it easier to retrace decisions or update models as new data becomes available—critical for compliance in regulated industries like finance or healthcare. Without a clear structure, data science efforts can become ad hoc, making results hard to reproduce and exposing the organization to risks such as model drift or incorrect decision-making."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f227d679-24f0-431c-9ab1-f1180650e7bc",
   "metadata": {},
   "source": [
    "## **Machine Learning**\n",
    "\n",
    "Machine Learning (ML) is a subfield of artificial intelligence (AI) that focuses on building systems that can learn from data and improve their performance over time without being explicitly programmed. In general terms, machine learning involves developing algorithms that can identify patterns, make decisions, or predict outcomes based on input data.\n",
    "\n",
    "<center><img src=\"https://www.edureka.co/blog/wp-content/uploads/2018/03/AI-vs-ML-vs-Deep-Learning.png\" width=\"700px\"></center>\n",
    "\n",
    "- **AI** is the science of making machines think and act like humans.\n",
    "- **ML** lets machines learn from data without being **explicitly programmed**.\n",
    "- **DL** uses neural networks with many layers to learn complex patterns.\n",
    "\n",
    "\"**Explicitly programmed**\" means giving a computer **exact, detailed instructions** on what to do in every possible situation.\n",
    "\n",
    "In traditional programming, you write rules like:\n",
    "\n",
    "```python\n",
    "if temperature > 30:\n",
    "    print(\"It's hot\")\n",
    "```\n",
    "\n",
    "The computer follows these exact rules and does not learn or adapt beyond them.\n",
    "\n",
    "In contrast, **machine learning** doesn't rely on predefined rules. Instead, it **learns the rules** by analyzing data. For example, instead of telling the computer what \"hot\" means, you show it many examples of temperatures labeled as \"hot\" or \"not hot,\" and it figures out the pattern."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e5c9085-8984-4285-8cd5-12e64d88ecd8",
   "metadata": {},
   "source": [
    "### **Different Approaches of Machine Learning**\n",
    "\n",
    "<center><img src=\"https://www.researchgate.net/publication/354960266/figure/fig1/AS:11431281251915131@1718389091562/The-main-types-of-machine-learning-Main-approaches-include-classification-and-regression.tif\" width=\"700px\"></center>\n",
    "\n",
    "**Supervised learning** is the most common type of machine learning, where the model is trained on **labeled data**. This means that for each input in the dataset, the correct output (or label) is provided, and the algorithm learns to map inputs to their corresponding outputs. Supervised learning is used for tasks like classification and regression.\n",
    "\n",
    "> **Example**:\n",
    "- **Classification**: Predicting whether an email is spam or not based on past labeled examples.\n",
    "- **Regression**: Predicting house prices based on features like size, location, and number of rooms.\n",
    "\n",
    "In this type of learning, the model is explicitly told what the \"correct\" answer is, and the goal is to minimize the difference between its predictions and the actual labels.\n",
    "\n",
    "\n",
    "**Unsupervised learning** works with **unlabeled data**. Here, the model tries to find patterns, structures, or relationships within the data without being given explicit output labels. This type of learning is useful for discovering hidden patterns or grouping data based on similarity, which is ideal for clustering and dimensionality reduction tasks.\n",
    "\n",
    "> **Example**:\n",
    "- **Clustering**: Grouping customers into different segments based on their purchasing behavior.\n",
    "- **Anomaly detection**: Identifying unusual transactions that could indicate fraud in financial data.\n",
    "\n",
    "Since there are no labels, unsupervised learning allows the model to explore the data and make inferences or classifications based on inherent structures.\n",
    "\n",
    "\n",
    "**Reinforcement learning** is a type of learning where an **agent** learns to make decisions by interacting with an **environment**. The agent receives feedback in the form of **rewards** or **penalties** based on its actions, and its goal is to maximize the cumulative reward over time. This type of learning is heavily used in fields like robotics, gaming, and autonomous driving.\n",
    "\n",
    "> **Example**:\n",
    "- **Game-playing**: Training an AI to play chess or Go by having it learn from playing games, where it receives rewards based on winning or losing.\n",
    "- **Robotics**: Teaching a robot to navigate through a maze by rewarding it for taking correct steps toward the goal.\n",
    "\n",
    "The key difference here is the agent’s interaction with the environment—learning from the consequences of its actions rather than being explicitly told what the correct behavior is."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6f523eb-8fd4-4140-9176-de90d7f8f529",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\"><b>Pondering Questions: </b>\n",
    "    \n",
    "1. *What do you think is ChatGPT's machine learning approach, and why is it effective for generating human-like responses?*\n",
    "2. *How does the quality and quantity of data affect the performance of AI tools like image generators or recommendation systems?*\n",
    "3. *In what ways might reinforcement learning be useful in real-world applications beyond games and robotics?*\n",
    "\n",
    "</div> "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d2909d7a-5e3c-4e80-8041-bdae3a00fdf9",
   "metadata": {},
   "source": [
    "### **Machine Learning Libraries and Frameworks**\n",
    "\n",
    "<center><img src=\"../figures/ml-tools2.jpg\" width=\"700px\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d6bcb6d-5316-4af5-8911-ae3e58ba354e",
   "metadata": {},
   "source": [
    "\n",
    "- **NumPy**. Provides support for arrays, matrices, and mathematical operations used in data preprocessing and model calculations.\n",
    "\n",
    "- **Pandas**. Offers structures like DataFrames, ideal for loading, cleaning, and exploring datasets before training a model.\n",
    "\n",
    "- **SciPy**. Useful for optimization, statistics, and signal processing during preprocessing or model evaluation steps.\n",
    "\n",
    "- **Scikit-learn (sklearn)**. A simple and powerful ML library for classical algorithms like decision trees, SVMs, and k-means.  \n",
    "\n",
    "- **TensorFlow**. Used to design, train, and deploy deep learning models at scale, especially for production environments.\n",
    "\n",
    "- **PyTorch**. Known for flexibility and ease of use in research; great for building custom neural networks and experimenting with model architectures."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa6b1c6e-4577-466f-b162-5ecdf2686698",
   "metadata": {},
   "source": [
    "### **ML Algorithms**\n",
    "\n",
    "There are various types of machine learning models, each designed with unique characteristics and suited for specific types of tasks. For instance, linear regression is simple and effective for predicting numerical values, while decision trees and random forests handle classification tasks with complex decision boundaries. Support Vector Machines (SVMs) work well for high-dimensional data, and K-nearest neighbors (KNN) is useful for intuitive, distance-based predictions. More complex models like neural networks are ideal for capturing intricate patterns in large datasets such as images or text. Each model has its strengths, assumptions, and limitations, making it important to understand when and how to use them appropriately.\n",
    "\n",
    "- https://scikit-learn.org/stable/supervised_learning.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d534a5fb-f53f-4ca9-88b9-456bee5882ab",
   "metadata": {},
   "source": [
    "## **ML Implementation**\n",
    "\n",
    "- Open the website below and `copy-paste` the codes.\n",
    "- https://scikit-learn.org/stable/auto_examples/classification/plot_digits_classification.html\n",
    "\n",
    "**Note:**\n",
    "\n",
    "- Make sure that `scikit-learn` and `matplotlib` is isntalled in your environments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd2632a8-7951-4263-b661-984d5bc9b775",
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy the codes here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c466890-0b04-43eb-ac55-b5af1d57fdd5",
   "metadata": {},
   "source": [
    "### **Export Model for Integration**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ef53037-4256-472f-b7c6-142c3389aaf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "joblib.dump(clf, 'svm_model.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
